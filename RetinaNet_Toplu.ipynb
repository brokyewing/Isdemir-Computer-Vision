{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb4ea48",
   "metadata": {},
   "source": [
    "Blok 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Environment & Backend Check (Auto Fallback)\n",
    "import sys, os, platform\n",
    "print(f\"Python: {sys.version.split()[0]}  OS: {platform.system()} {platform.release()}\")\n",
    "\n",
    "DETECTRON2_AVAILABLE = False\n",
    "try:\n",
    "    import detectron2  # type: ignore\n",
    "    DETECTRON2_AVAILABLE = True\n",
    "    print(\"‚úÖ Detectron2 available - primary pipeline can run\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Detectron2 not available: {e}\")\n",
    "    print(\"‚û°Ô∏è Will use TorchVision RetinaNet fallback pipeline\")\n",
    "\n",
    "# Core deps for fallback\n",
    "%pip -q install --upgrade pillow numpy matplotlib opencv-python-headless pytesseract tqdm pyyaml torchvision torch || true\n",
    "\n",
    "# Tesseract (Colab)\n",
    "try:\n",
    "    import subprocess\n",
    "    if platform.system() == 'Linux':\n",
    "        subprocess.run(['apt','update','-y'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        subprocess.run(['apt','install','-y','tesseract-ocr'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print('‚úÖ Tesseract ready')\n",
    "except Exception as _:\n",
    "    print('‚ö†Ô∏è Tesseract install skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693b4f6",
   "metadata": {},
   "source": [
    "Blok 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Dataset Resolve (COCO zip)\n",
    "import glob, json, zipfile, pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "COCO_ZIP = \"/content/Slab 3.v6i.coco.zip\"\n",
    "DATA_ROOT = Path(\"/content/data_coco\")\n",
    "\n",
    "if not Path(COCO_ZIP).exists():\n",
    "    raise FileNotFoundError(f\"Zip not found: {COCO_ZIP}\")\n",
    "\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "with zipfile.ZipFile(COCO_ZIP, 'r') as z:\n",
    "    z.extractall(DATA_ROOT)\n",
    "\n",
    "def _find_first(pattern):\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    return files[0] if files else None\n",
    "\n",
    "TRAIN_JSON = _find_first(f\"{DATA_ROOT}/**/train/_annotations.coco.json\")\n",
    "VAL_JSON   = _find_first(f\"{DATA_ROOT}/**/valid/_annotations.coco.json\")\n",
    "TEST_JSON  = _find_first(f\"{DATA_ROOT}/**/test/_annotations.coco.json\")\n",
    "assert TRAIN_JSON and VAL_JSON, \"train/valid COCO json bulunamadƒ±\"\n",
    "\n",
    "TRAIN_DIR = str(pathlib.Path(TRAIN_JSON).parent)\n",
    "VAL_DIR   = str(pathlib.Path(VAL_JSON).parent)\n",
    "TEST_DIR  = str(pathlib.Path(TEST_JSON).parent) if TEST_JSON else VAL_DIR\n",
    "\n",
    "print(\"TRAIN:\", TRAIN_DIR)\n",
    "print(\"VALID:\", VAL_DIR)\n",
    "print(\"TEST :\", TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dcc9ed",
   "metadata": {},
   "source": [
    "Blok 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Digits -> Textline COCO (with transcript)\n",
    "import numpy as np\n",
    "\n",
    "def coco_digits_to_textlines(src_json, dst_json, y_tol_ratio=0.06, gap_ratio=0.08):\n",
    "    coco = json.load(open(src_json))\n",
    "    id2img = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "    id2name = {c[\"id\"]: str(c.get(\"name\", c[\"id\"])) for c in coco.get(\"categories\", [])}\n",
    "\n",
    "    anns_by_img = {}\n",
    "    for ann in coco[\"annotations\"]:\n",
    "        if ann.get(\"iscrowd\",0)==1:\n",
    "            continue\n",
    "        anns_by_img.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "    new = {\"images\": coco[\"images\"], \"type\":\"instances\",\n",
    "           \"categories\":[{\"id\":1,\"name\":\"textline\"}], \"annotations\":[]}\n",
    "    ann_id = 1\n",
    "\n",
    "    for img_id, anns in anns_by_img.items():\n",
    "        if not anns:\n",
    "            continue\n",
    "        H = id2img[img_id][\"height\"]\n",
    "\n",
    "        boxes, digits = [], []\n",
    "        for a in anns:\n",
    "            x,y,w,h = a[\"bbox\"]\n",
    "            boxes.append([x,y,w,h])\n",
    "            name = id2name.get(a[\"category_id\"], str(a[\"category_id\"]))\n",
    "            try: d = str(int(name))\n",
    "            except: d = str(a[\"category_id\"])\n",
    "            digits.append(d)\n",
    "\n",
    "        boxes = np.array(boxes, float)\n",
    "        if len(boxes)==0:\n",
    "            continue\n",
    "\n",
    "        y_ctr = boxes[:,1] + boxes[:,3]/2\n",
    "        x_ctr = boxes[:,0] + boxes[:,2]/2\n",
    "        idx = np.argsort(y_ctr)\n",
    "        rows, row = [], [idx[0]]\n",
    "        y_tol = y_tol_ratio * H\n",
    "        for i in idx[1:]:\n",
    "            if abs(y_ctr[i]-y_ctr[row[-1]]) <= y_tol:\n",
    "                row.append(i)\n",
    "            else:\n",
    "                rows.append(row); row=[i]\n",
    "        rows.append(row)\n",
    "\n",
    "        for r in rows:\n",
    "            r_sorted = sorted(r, key=lambda i: x_ctr[i])\n",
    "            xs = [x_ctr[i] for i in r_sorted]\n",
    "            gap_thr = float('inf') if len(xs)<=1 else gap_ratio*(max(xs)-min(xs)+1e-6)\n",
    "\n",
    "            groups=[]; grp=[r_sorted[0]]\n",
    "            for k in range(1,len(r_sorted)):\n",
    "                if xs[k]-xs[k-1] > gap_thr:\n",
    "                    groups.append(grp); grp=[r_sorted[k]]\n",
    "                else:\n",
    "                    grp.append(r_sorted[k])\n",
    "            groups.append(grp)\n",
    "\n",
    "            for g in groups:\n",
    "                x0 = float(np.min(boxes[g][:,0])); y0 = float(np.min(boxes[g][:,1]))\n",
    "                x1 = float(np.max(boxes[g][:,0]+boxes[g][:,2])); y1 = float(np.max(boxes[g][:,1]+boxes[g][:,3]))\n",
    "                w  = x1-x0; h = y1-y0\n",
    "                transcript = \"\".join(digits[i] for i in g)\n",
    "\n",
    "                new[\"annotations\"].append({\n",
    "                    \"id\": ann_id, \"image_id\": img_id, \"category_id\": 1,\n",
    "                    \"bbox\":[x0,y0,w,h], \"area\":float(w*h), \"iscrowd\":0,\n",
    "                    \"transcript\": transcript\n",
    "                })\n",
    "                ann_id += 1\n",
    "\n",
    "    json.dump(new, open(dst_json, \"w\"))\n",
    "    print(f\"[OK] wrote {dst_json}  anns={len(new['annotations'])}\")\n",
    "\n",
    "TRAIN_TL_JSON = str(pathlib.Path(TRAIN_JSON).parent / \"_annotations_textline.coco.json\")\n",
    "VAL_TL_JSON   = str(pathlib.Path(VAL_JSON).parent   / \"_annotations_textline.coco.json\")\n",
    "if not os.path.exists(TRAIN_TL_JSON): coco_digits_to_textlines(TRAIN_JSON, TRAIN_TL_JSON)\n",
    "else: print(\"[SKIP] train textline JSON zaten var\")\n",
    "if not os.path.exists(VAL_TL_JSON):   coco_digits_to_textlines(VAL_JSON,   VAL_TL_JSON)\n",
    "else: print(\"[SKIP] valid textline JSON zaten var\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd3bc4",
   "metadata": {},
   "source": [
    "Blok 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TorchVision Fallback: train + infer + OCR + export =====\n",
    "if not DETECTRON2_AVAILABLE:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torchvision.models.detection import retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
    "    from torchvision.transforms import functional as F\n",
    "    import cv2, numpy as np, json, os, pathlib\n",
    "    from PIL import Image\n",
    "    from tqdm import tqdm\n",
    "    import pytesseract\n",
    "\n",
    "    class CocoTextlineDataset(Dataset):\n",
    "        def __init__(self, root_dir, split, ann_name='_annotations_textline.coco.json'):\n",
    "            self.root_dir = pathlib.Path(root_dir)\n",
    "            self.split = split\n",
    "            with open(self.root_dir/ann_name, 'r', encoding='utf-8') as f:\n",
    "                coco = json.load(f)\n",
    "            self.images = coco['images']\n",
    "            self.anns = coco['annotations']\n",
    "            self.img2anns = {}\n",
    "            for a in self.anns:\n",
    "                self.img2anns.setdefault(a['image_id'], []).append(a)\n",
    "            self.id2file = {im['id']: im['file_name'] for im in self.images}\n",
    "        def __len__(self): return len(self.images)\n",
    "        def __getitem__(self, idx):\n",
    "            iminfo = self.images[idx]\n",
    "            p = pathlib.Path(TRAIN_DIR if self.split=='train' else VAL_DIR)/iminfo['file_name']\n",
    "            img = Image.open(p).convert('RGB')\n",
    "            anns = self.img2anns.get(iminfo['id'], [])\n",
    "            boxes, labels = [], []\n",
    "            for a in anns:\n",
    "                x,y,w,h = a['bbox']\n",
    "                if w>1 and h>1:\n",
    "                    boxes.append([x,y,x+w,y+h]); labels.append(0)\n",
    "            if not boxes:\n",
    "                boxes = np.zeros((0,4), dtype=np.float32)\n",
    "                labels = np.zeros((0,), dtype=np.int64)\n",
    "            target={'boxes':torch.as_tensor(boxes,dtype=torch.float32),\n",
    "                    'labels':torch.as_tensor(labels,dtype=torch.int64)}\n",
    "            return F.to_tensor(img), target\n",
    "    def collate_fn(b): return tuple(zip(*b))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_ds = CocoTextlineDataset(TRAIN_DIR, 'train')\n",
    "    val_ds   = CocoTextlineDataset(VAL_DIR,   'valid')\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Build model with correct num_classes (no background class in RetinaNet)\n",
    "    model = retinanet_resnet50_fpn(weights=None, num_classes=1)\n",
    "    print('RetinaNet num_classes =', getattr(model.head.classification_head, 'num_classes', 'unknown'))\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    def epoch(model, loader, train=True):\n",
    "        model.train(); tot=0.0\n",
    "        for imgs,tgts in tqdm(loader, desc=('train' if train else 'valid')):\n",
    "            imgs=[i.to(device) for i in imgs]\n",
    "            tgts=[{k:v.to(device) for k,v in t.items()} for t in tgts]\n",
    "            lossd=model(imgs,tgts); loss=sum(lossd.values())\n",
    "            if train:\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tot+=loss.item()\n",
    "        return tot/max(1,len(loader))\n",
    "\n",
    "    best=float('inf')\n",
    "    for e in range(1,51):\n",
    "        tl=epoch(model,train_loader,True); vl=epoch(model,val_loader,False)\n",
    "        print(f\"E{e:03d} tl={tl:.4f} vl={vl:.4f}\")\n",
    "        if vl<best:\n",
    "            best=vl; torch.save(model.state_dict(), '/content/retina_textline_best.pth')\n",
    "            print('saved best')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(img_path, th=0.3):\n",
    "        model.eval()\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        t=F.to_tensor(img).to(device)\n",
    "        out=model([t])[0]\n",
    "        b=out['boxes'].cpu().numpy(); s=out['scores'].cpu().numpy()\n",
    "        keep=s>=th\n",
    "        return b[keep], s[keep], np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80daf069",
   "metadata": {},
   "source": [
    "Blok 4 (22.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Interactive image picker ‚Üí RetinaNet(best) + OCR\n",
    "# Works in Jupyter/Colab using ipywidgets. It lists images from TEST_DIR and VALID_DIR.\n",
    "\n",
    "import glob\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions for OCR and drawing (error tolerance)\n",
    "def yellow_pre(bgr):\n",
    "    hsv=cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
    "    lower=np.array([15,60,120]); upper=np.array([40,255,255])\n",
    "    mask=cv2.inRange(hsv, lower, upper); inv=cv2.bitwise_not(mask)\n",
    "    fg=cv2.bitwise_and(bgr,bgr,mask=inv)\n",
    "    g=cv2.cvtColor(fg, cv2.COLOR_BGR2GRAY)\n",
    "    g=cv2.medianBlur(g,3); g=cv2.normalize(g,None,0,255,cv2.NORM_MINMAX)\n",
    "    return g\n",
    "\n",
    "def ocr(g):\n",
    "    cfg='--oem 1 --psm 7 -c tessedit_char_whitelist=0123456789'\n",
    "    txt=pytesseract.image_to_string(g, config=cfg)\n",
    "    txt=''.join(ch for ch in txt if ch.isdigit())\n",
    "    data=pytesseract.image_to_data(g,config=cfg,output_type=pytesseract.Output.DICT)\n",
    "    conf=[int(c) for c in data.get('conf',[]) if str(c).isdigit() and int(c)>=0]\n",
    "    m=(np.mean(conf)/100.0) if conf else 0.0\n",
    "    return txt,m\n",
    "    \n",
    "def draw_box_with_alpha(img_bgr, x1, y1, x2, y2, color=(0,200,0), alpha=0.75, label_text=None):\n",
    "    overlay = img_bgr.copy()\n",
    "    cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)\n",
    "    cv2.addWeighted(overlay, alpha, img_bgr, 1 - alpha, 0, img_bgr)\n",
    "    cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 2)\n",
    "    if label_text:\n",
    "        cv2.putText(img_bgr, label_text, (x1, max(16, y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "CONF_THRESH = 0.50\n",
    "LABEL_MODE = \"ocr\"\n",
    "\n",
    "# gather images\n",
    "_candidates = []\n",
    "for d in [TEST_DIR, VAL_DIR, TRAIN_DIR]:\n",
    "    if d:\n",
    "        _c = glob.glob(f\"{d}/*.*\")\n",
    "        for p in _c:\n",
    "            if p.lower().endswith(('.jpg','.jpeg','.png','.bmp','.tif','.tiff')):\n",
    "                _candidates.append(p)\n",
    "\n",
    "_candi_sorted = sorted(set(_candidates))\n",
    "if not _candi_sorted:\n",
    "    print(\"No images found in TEST/VALID/TRAIN directories.\")\n",
    "else:\n",
    "    dd = widgets.Dropdown(options=_candi_sorted, description='Image:')\n",
    "    btn = widgets.Button(description='Run detection + OCR', button_style='success')\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def _on_click(_):\n",
    "        out.clear_output()\n",
    "        with out:\n",
    "            img_path = dd.value\n",
    "            print(\"Selected:\", img_path)\n",
    "            model.load_state_dict(torch.load('/content/retina_textline_best.pth', map_location=device))\n",
    "            model.eval()\n",
    "            boxes, scores, rgb = infer(img_path, th=CONF_THRESH)\n",
    "            bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "            results = []\n",
    "            for bx, sc in zip(boxes, scores):\n",
    "                x1, y1, x2, y2 = map(int, bx)\n",
    "                roi = bgr[max(0,y1):y2, max(0,x1):x2]\n",
    "                if roi.size == 0:\n",
    "                    continue\n",
    "                g = yellow_pre(roi)\n",
    "                txt, mconf = ocr(g)\n",
    "                results.append({'box': bx.tolist(), 'det': float(sc), 'text': txt, 'conf': float(mconf)})\n",
    "                label = f\"{int(sc*100)}%\" if LABEL_MODE==\"confidence\" else txt\n",
    "                draw_box_with_alpha(bgr, x1, y1, x2, y2, color=(0,200,0), label_text=label)\n",
    "\n",
    "            from pathlib import Path\n",
    "            OUT_ANNOT = str(Path(img_path).with_name(Path(img_path).stem + \"_annot.jpg\"))\n",
    "            cv2.imwrite(OUT_ANNOT, bgr)\n",
    "            results.sort(key=lambda r: r['box'][0])\n",
    "            print(\"Saved:\", OUT_ANNOT)\n",
    "            print(\"Detections:\", len(results))\n",
    "            print(results[:10])\n",
    "\n",
    "            # inline preview (optional)\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure(figsize=(12,8))\n",
    "                plt.imshow(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    btn.on_click(_on_click)\n",
    "    display(widgets.VBox([dd, btn, out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339d0e",
   "metadata": {},
   "source": [
    "Blok 5(22.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ccea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Two-stage pipeline: RetinaNet boxes -> Tesseract OCR inside boxes (batch)\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def retinanet_tesseract_pipeline(input_dir, out_dir=\"retina_ocr_out\", det_th=0.30, pad=4, use_yellow=True, save_annot=True):\n",
    "    input_dir = Path(input_dir)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ann_dir = out_dir / \"annotated\"; ann_dir.mkdir(exist_ok=True)\n",
    "    csv_path = out_dir / \"results.csv\"\n",
    "\n",
    "    # ensure weights loaded\n",
    "    model.load_state_dict(torch.load('/content/retina_textline_best.pth', map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"image\",\"det_id\",\"x1\",\"y1\",\"x2\",\"y2\",\"det_score\",\"ocr_text\",\"ocr_conf\"])\n",
    "\n",
    "        for img_path in sorted(input_dir.glob('*.*')):\n",
    "            if img_path.suffix.lower() not in [\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"]:\n",
    "                continue\n",
    "            bgr = cv2.imread(str(img_path))\n",
    "            if bgr is None:\n",
    "                continue\n",
    "            H, W = bgr.shape[:2]\n",
    "\n",
    "            boxes, scores = detect_boxes_retinanet(bgr, det_th=det_th)\n",
    "            order = np.argsort(boxes[:, 0]) if len(boxes) else []\n",
    "            drawn = bgr.copy()\n",
    "\n",
    "            for det_id, idx in enumerate(order):\n",
    "                x1, y1, x2, y2 = boxes[idx].astype(int)\n",
    "                x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)\n",
    "                x2 = min(W, x2 + pad); y2 = min(H, y2 + pad)\n",
    "                roi = bgr[y1:y2, x1:x2]\n",
    "                if roi.size == 0:\n",
    "                    continue\n",
    "\n",
    "                gray = yellow_pre(roi) if use_yellow else cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "                text, mconf = ocr(gray)\n",
    "\n",
    "                w.writerow([img_path.name, det_id, x1, y1, x2, y2, float(scores[idx]), text, round(float(mconf), 3)])\n",
    "\n",
    "                if save_annot:\n",
    "                    color = (0, 200, 0)\n",
    "                    label = f\"{int(scores[idx]*100)}%\" if LABEL_MODE==\"confidence\" else text\n",
    "                    draw_box_with_alpha(drawn, x1, y1, x2, y2, color=color, label_text=label)\n",
    "\n",
    "            if save_annot:\n",
    "                cv2.imwrite(str(ann_dir / img_path.name), drawn)\n",
    "\n",
    "    return str(csv_path)\n",
    "\n",
    "# Example usage (edit input folder):\n",
    "# csv_file = retinanet_tesseract_pipeline(input_dir=TEST_DIR, out_dir=\"/content/retina_ocr_out\", det_th=0.35)\n",
    "# print(\"CSV:\", csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d89c8",
   "metadata": {},
   "source": [
    "Blok 4(25.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Interactive image picker ‚Üí RetinaNet(best) + OCR\n",
    "# Works in Jupyter/Colab using ipywidgets. It lists images from TEST_DIR and VALID_DIR.\n",
    "\n",
    "import glob\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions for OCR and drawing (error tolerance)\n",
    "def preprocess_for_ocr(roi_bgr):\n",
    "    gray = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    contrast_enhanced = clahe.apply(gray)\n",
    "    thresh = cv2.adaptiveThreshold(contrast_enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    kernel = np.ones((1,1), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "    return closing\n",
    "\n",
    "def ocr(g):\n",
    "    cfg='--oem 1 --psm 7 -c tessedit_char_whitelist=0123456789'\n",
    "    txt=pytesseract.image_to_string(g, config=cfg)\n",
    "    txt=''.join(ch for ch in txt if ch.isdigit())\n",
    "    data=pytesseract.image_to_data(g,config=cfg,output_type=pytesseract.Output.DICT)\n",
    "    conf=[int(c) for c in data.get('conf',[]) if str(c).isdigit() and int(c)>=0]\n",
    "    m=(np.mean(conf)/100.0) if conf else 0.0\n",
    "    return txt,m\n",
    "\n",
    "def draw_box_with_alpha(img_bgr, x1, y1, x2, y2, color=(0,200,0), alpha=0.75, label_text=None):\n",
    "    overlay = img_bgr.copy()\n",
    "    cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)\n",
    "    cv2.addWeighted(overlay, alpha, img_bgr, 1 - alpha, 0, img_bgr)\n",
    "    cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 2)\n",
    "    if label_text:\n",
    "        cv2.putText(img_bgr, label_text, (x1, max(16, y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "CONF_THRESH = 0.50\n",
    "LABEL_MODE = \"ocr\"\n",
    "\n",
    "# gather images\n",
    "_candidates = []\n",
    "for d in [TEST_DIR, VAL_DIR, TRAIN_DIR]:\n",
    "    if d:\n",
    "        _c = glob.glob(f\"{d}/*.*\")\n",
    "        for p in _c:\n",
    "            if p.lower().endswith(('.jpg','.jpeg','.png','.bmp','.tif','.tiff')):\n",
    "                _candidates.append(p)\n",
    "\n",
    "_candi_sorted = sorted(set(_candidates))\n",
    "if not _candi_sorted:\n",
    "    print(\"No images found in TEST/VALID/TRAIN directories.\")\n",
    "else:\n",
    "    dd = widgets.Dropdown(options=_candi_sorted, description='Image:')\n",
    "    btn = widgets.Button(description='Run detection + OCR', button_style='success')\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def _on_click(_):\n",
    "        out.clear_output()\n",
    "        with out:\n",
    "            img_path = dd.value\n",
    "            print(\"Selected:\", img_path)\n",
    "            model.load_state_dict(torch.load('/content/retina_textline_best.pth', map_location=device))\n",
    "            model.eval()\n",
    "            boxes, scores, rgb = infer(img_path, th=CONF_THRESH)\n",
    "            bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "            results = []\n",
    "            for bx, sc in zip(boxes, scores):\n",
    "                x1, y1, x2, y2 = map(int, bx)\n",
    "                roi = bgr[max(0,y1):y2, max(0,x1):x2]\n",
    "                if roi.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Apply the new preprocessing steps\n",
    "                preprocessed_roi = preprocess_for_ocr(roi)\n",
    "\n",
    "                txt, mconf = ocr(preprocessed_roi)\n",
    "                results.append({'box': bx.tolist(), 'det': float(sc), 'text': txt, 'conf': float(mconf)})\n",
    "                label = f\"{int(sc*100)}%\" if LABEL_MODE==\"confidence\" else txt\n",
    "                draw_box_with_alpha(bgr, x1, y1, x2, y2, color=(0,200,0), label_text=label)\n",
    "\n",
    "            from pathlib import Path\n",
    "            OUT_ANNOT = str(Path(img_path).with_name(Path(img_path).stem + \"_annot.jpg\"))\n",
    "            cv2.imwrite(OUT_ANNOT, bgr)\n",
    "            results.sort(key=lambda r: r['box'][0])\n",
    "            print(\"Saved:\", OUT_ANNOT)\n",
    "            print(\"Detections:\", len(results))\n",
    "            print(results[:10])\n",
    "\n",
    "            # inline preview (optional)\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure(figsize=(12,8))\n",
    "                plt.imshow(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    btn.on_click(_on_click)\n",
    "    display(widgets.VBox([dd, btn, out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a87f5",
   "metadata": {},
   "source": [
    "Blok 5(25.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Two-stage pipeline: RetinaNet boxes -> Tesseract OCR inside boxes (batch)\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# The preprocess_for_ocr function from the block above should be accessible here.\n",
    "# If running this cell independently, redefine it.\n",
    "def preprocess_for_ocr(roi_bgr):\n",
    "    gray = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    contrast_enhanced = clahe.apply(gray)\n",
    "    thresh = cv2.adaptiveThreshold(contrast_enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    kernel = np.ones((1,1), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    return closing\n",
    "\n",
    "def retinanet_tesseract_pipeline(input_dir, out_dir=\"retina_ocr_out\", det_th=0.30, pad=4, save_annot=True):\n",
    "    input_dir = Path(input_dir)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ann_dir = out_dir / \"annotated\"; ann_dir.mkdir(exist_ok=True)\n",
    "    csv_path = out_dir / \"results.csv\"\n",
    "\n",
    "    # ensure weights loaded\n",
    "    model.load_state_dict(torch.load('/content/retina_textline_best.pth', map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"image\",\"det_id\",\"x1\",\"y1\",\"x2\",\"y2\",\"det_score\",\"ocr_text\",\"ocr_conf\"])\n",
    "\n",
    "        image_paths = sorted([p for p in input_dir.glob('*.*') if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"]])\n",
    "\n",
    "        for img_path in tqdm(image_paths, desc=\"Processing Images\"):\n",
    "            bgr = cv2.imread(str(img_path))\n",
    "            if bgr is None:\n",
    "                continue\n",
    "            H, W = bgr.shape[:2]\n",
    "\n",
    "            # Re-using the infer function for consistency\n",
    "            boxes, scores, _ = infer(str(img_path), th=det_th)\n",
    "\n",
    "            order = np.argsort(boxes[:, 0]) if len(boxes) > 0 else []\n",
    "            drawn = bgr.copy()\n",
    "\n",
    "            for det_id, idx in enumerate(order):\n",
    "                x1, y1, x2, y2 = boxes[idx].astype(int)\n",
    "                x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)\n",
    "                x2 = min(W, x2 + pad); y2 = min(H, y2 + pad)\n",
    "                roi = bgr[y1:y2, x1:x2]\n",
    "                if roi.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Apply the advanced preprocessing steps\n",
    "                preprocessed_roi = preprocess_for_ocr(roi)\n",
    "                text, mconf = ocr(preprocessed_roi)\n",
    "\n",
    "                w.writerow([img_path.name, det_id, x1, y1, x2, y2, float(scores[idx]), text, round(float(mconf), 3)])\n",
    "\n",
    "                if save_annot:\n",
    "                    color = (0, 200, 0)\n",
    "                    label = f\"{int(scores[idx]*100)}%\" if LABEL_MODE==\"confidence\" else text\n",
    "                    draw_box_with_alpha(drawn, x1, y1, x2, y2, color=color, label_text=label)\n",
    "\n",
    "            if save_annot:\n",
    "                cv2.imwrite(str(ann_dir / img_path.name), drawn)\n",
    "\n",
    "    return str(csv_path)\n",
    "\n",
    "# Example usage (edit input folder):\n",
    "# from tqdm.notebook import tqdm # Use notebook-friendly progress bar\n",
    "# csv_file = retinanet_tesseract_pipeline(input_dir=TEST_DIR, out_dir=\"/content/retina_ocr_out_v2\", det_th=0.35)\n",
    "# print(\"CSV:\", csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263f65a",
   "metadata": {},
   "source": [
    "Blok 4(26.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Interactive image picker ‚Üí RetinaNet(best) + OCR\n",
    "# Works in Jupyter/Colab using ipywidgets. It lists images from TEST_DIR and VALID_DIR.\n",
    "\n",
    "import glob\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions for OCR and drawing (error tolerance)\n",
    "def preprocess_and_segment_digits(roi_bgr, min_area_ratio=0.01, aspect_ratio_range=(0.1, 2.5)):\n",
    "    gray = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_enhanced = clahe.apply(gray)\n",
    "    thresh = cv2.adaptiveThreshold(contrast_enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(closing, 4, cv2.CV_32S)\n",
    "    digit_images = []\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        total_roi_area = roi_bgr.shape[0] * roi_bgr.shape[1]\n",
    "        if area < total_roi_area * min_area_ratio: continue\n",
    "        aspect_ratio = w / h\n",
    "        if not (aspect_ratio_range[0] < aspect_ratio < aspect_ratio_range[1]): continue\n",
    "        digit_roi = closing[y:y+h, x:x+w]\n",
    "        padded_digit = cv2.copyMakeBorder(digit_roi, 10, 10, 10, 10, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "        digit_images.append((x, padded_digit))\n",
    "\n",
    "    digit_images.sort(key=lambda item: item[0])\n",
    "    return [img for x, img in digit_images]\n",
    "\n",
    "def ocr_single_digit(digit_image):\n",
    "    cfg = '--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789'\n",
    "    txt = pytesseract.image_to_string(digit_image, config=cfg)\n",
    "    txt = ''.join(ch for ch in txt if ch.isdigit())\n",
    "    conf = 100 if txt else 0\n",
    "    return txt, conf / 100.0\n",
    "\n",
    "def draw_box_with_alpha(img_bgr, x1, y1, x2, y2, color=(0,200,0), alpha=0.75, label_text=None):\n",
    "    overlay = img_bgr.copy()\n",
    "    cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)\n",
    "    cv2.addWeighted(overlay, alpha, img_bgr, 1 - alpha, 0, img_bgr)\n",
    "    cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 2)\n",
    "    if label_text:\n",
    "        cv2.putText(img_bgr, label_text, (x1, max(16, y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "CONF_THRESH = 0.50\n",
    "LABEL_MODE = \"ocr\"\n",
    "\n",
    "# gather images\n",
    "_candidates = sorted(set(glob.glob(f\"{TEST_DIR}/*.*\") + glob.glob(f\"{VAL_DIR}/*.*\")))\n",
    "if not _candidates:\n",
    "    print(\"No images found.\")\n",
    "else:\n",
    "    dd = widgets.Dropdown(options=_candidates, description='Image:')\n",
    "    btn = widgets.Button(description='Run detection + OCR', button_style='success')\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def _on_click(_):\n",
    "        out.clear_output()\n",
    "        with out:\n",
    "            img_path = dd.value\n",
    "            print(\"Selected:\", img_path)\n",
    "            model.load_state_dict(torch.load('/content/retina_textline_best.pth', map_location=device))\n",
    "            model.eval()\n",
    "            boxes, scores, rgb = infer(img_path, th=CONF_THRESH)\n",
    "            bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "            results = []\n",
    "            \n",
    "            # ===== G√úNCELLENMƒ∞≈û ANA D√ñNG√ú =====\n",
    "            for bx, sc in zip(boxes, scores):\n",
    "                x1, y1, x2, y2 = map(int, bx)\n",
    "                roi = bgr[max(0, y1):y2, max(0, x1):x2]\n",
    "                if roi.size == 0: continue\n",
    "\n",
    "                segmented_digit_images = preprocess_and_segment_digits(roi)\n",
    "                \n",
    "                full_text = \"\"\n",
    "                confidences = []\n",
    "                for digit_img in segmented_digit_images:\n",
    "                    digit_txt, digit_conf = ocr_single_digit(digit_img)\n",
    "                    full_text += digit_txt\n",
    "                    if digit_conf > 0: confidences.append(digit_conf)\n",
    "                \n",
    "                avg_conf = np.mean(confidences) if confidences else 0.0\n",
    "\n",
    "                results.append({'box': bx.tolist(), 'det': float(sc), 'text': full_text, 'conf': float(avg_conf)})\n",
    "                label = f\"{int(sc*100)}%\" if LABEL_MODE == \"confidence\" else full_text\n",
    "                draw_box_with_alpha(bgr, x1, y1, x2, y2, color=(0, 200, 0), label_text=label)\n",
    "\n",
    "            OUT_ANNOT = str(Path(img_path).with_name(Path(img_path).stem + \"_annot.jpg\"))\n",
    "            cv2.imwrite(OUT_ANNOT, bgr)\n",
    "            results.sort(key=lambda r: r['box'][0])\n",
    "            print(\"Saved:\", OUT_ANNOT)\n",
    "            print(\"Detections:\", len(results))\n",
    "            print(results)\n",
    "\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(12,8))\n",
    "            plt.imshow(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off'); plt.show()\n",
    "\n",
    "    btn.on_click(_on_click)\n",
    "    display(widgets.VBox([dd, btn, out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad90a3b",
   "metadata": {},
   "source": [
    "Blok 5(26.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Two-stage pipeline: RetinaNet boxes -> Tesseract OCR inside boxes (batch)\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Not: Gerekli fonksiyonlar bir √∂nceki h√ºcrede zaten tanƒ±mlƒ± olduƒüu i√ßin burada tekrar tanƒ±mlamaya gerek yok.\n",
    "# Eƒüer bu h√ºcreyi baƒüƒ±msƒ±z √ßalƒ±≈ütƒ±racaksanƒ±z, preprocess_and_segment_digits ve ocr_single_digit fonksiyonlarƒ±nƒ±\n",
    "# buraya da kopyalamanƒ±z gerekir.\n",
    "\n",
    "def retinanet_tesseract_pipeline(input_dir, out_dir=\"retina_ocr_out\", det_th=0.30, pad=4, save_annot=True):\n",
    "    input_dir = Path(input_dir)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ann_dir = out_dir / \"annotated\"; ann_dir.mkdir(exist_ok=True)\n",
    "    csv_path = out_dir / \"results.csv\"\n",
    "\n",
    "    model.load_state_dict(torch.load('/content/retina_textline_best.pth', map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"image\",\"det_id\",\"x1\",\"y1\",\"x2\",\"y2\",\"det_score\",\"ocr_text\",\"ocr_conf\"])\n",
    "\n",
    "        image_paths = sorted([p for p in input_dir.glob('*.*') if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"]])\n",
    "\n",
    "        for img_path in tqdm(image_paths, desc=\"Processing Images\"):\n",
    "            bgr = cv2.imread(str(img_path))\n",
    "            if bgr is None: continue\n",
    "            H, W = bgr.shape[:2]\n",
    "\n",
    "            boxes, scores, _ = infer(str(img_path), th=det_th)\n",
    "            order = np.argsort(boxes[:, 0]) if len(boxes) > 0 else []\n",
    "            drawn = bgr.copy()\n",
    "\n",
    "            # ===== G√úNCELLENMƒ∞≈û ANA D√ñNG√ú =====\n",
    "            for det_id, idx in enumerate(order):\n",
    "                x1, y1, x2, y2 = boxes[idx].astype(int)\n",
    "                x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)\n",
    "                x2 = min(W, x2 + pad); y2 = min(H, y2 + pad)\n",
    "                roi = bgr[y1:y2, x1:x2]\n",
    "                if roi.size == 0: continue\n",
    "\n",
    "                segmented_digit_images = preprocess_and_segment_digits(roi)\n",
    "                full_text = \"\"\n",
    "                confidences = []\n",
    "                for digit_img in segmented_digit_images:\n",
    "                    digit_txt, digit_conf = ocr_single_digit(digit_img)\n",
    "                    full_text += digit_txt\n",
    "                    if digit_conf > 0: confidences.append(digit_conf)\n",
    "                \n",
    "                avg_conf = np.mean(confidences) if confidences else 0.0\n",
    "\n",
    "                w.writerow([img_path.name, det_id, x1, y1, x2, y2, float(scores[idx]), full_text, round(float(avg_conf), 3)])\n",
    "\n",
    "                if save_annot:\n",
    "                    label = f\"{int(scores[idx]*100)}%\" if LABEL_MODE == \"confidence\" else full_text\n",
    "                    draw_box_with_alpha(drawn, x1, y1, x2, y2, color=(0, 200, 0), label_text=label)\n",
    "\n",
    "            if save_annot:\n",
    "                cv2.imwrite(str(ann_dir / img_path.name), drawn)\n",
    "\n",
    "    return str(csv_path)\n",
    "\n",
    "# √ñrnek Kullanƒ±m:\n",
    "# from tqdm.notebook import tqdm\n",
    "# csv_file = retinanet_tesseract_pipeline(input_dir=TEST_DIR, out_dir=\"/content/retina_ocr_out_v2\", det_th=0.35)\n",
    "# print(\"CSV:\", csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
